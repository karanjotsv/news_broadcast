{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "week2 objectives:\n",
    "1. clean transcript to get text with timestamps\n",
    "2. merge phrases to form sentences\n",
    "3. merge sentences to form chunks having a minimum duration\n",
    "4. keyword extraction from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../w1/meta/transcript.json\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Good evening and welcome to Tucker Carlson tonight.',\n",
       "  'start': 0.0,\n",
       "  'end': 3.56},\n",
       " {'text': \"If there's one thing your average liberal understands perfectly well, it's that there's\",\n",
       "  'start': 3.56,\n",
       "  'end': 7.96},\n",
       " {'text': 'safety in numbers.', 'start': 7.96, 'end': 10.08},\n",
       " {'text': \"Don't go out alone.\", 'start': 10.08, 'end': 11.48},\n",
       " {'text': 'Bring 80 million people with you.', 'start': 11.48, 'end': 13.28},\n",
       " {'text': \"It's safer that way.\", 'start': 13.28, 'end': 15.2},\n",
       " {'text': 'There is a reason, a fundamental reason, that Democrats are natural joiners and organizers',\n",
       "  'start': 15.2,\n",
       "  'end': 20.54},\n",
       " {'text': 'and petition signers and that their highest virtue is conformity.',\n",
       "  'start': 20.54,\n",
       "  'end': 25.86},\n",
       " {'text': \"They know that as long as they're all wearing the same uniform, they'll probably be okay.\",\n",
       "  'start': 25.86,\n",
       "  'end': 30.74},\n",
       " {'text': \"This is why you'll see one person in Brookline or Bethesda raise a Ukrainian flag in the\",\n",
       "  'start': 30.74,\n",
       "  'end': 35.2}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = [{'text': i['text'].strip(), 'start': round(i['start'], 3), 'end': round(i['end'], 3)} for i in data['de_fnc.mp4']]\n",
    "\n",
    "phrases[ : 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"phrase.json\", 'w') as f:\n",
    "    json.dump(phrases, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge phrases to form sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(segments):\n",
    "    '''\n",
    "    merge segments to sentences\n",
    "    '''\n",
    "    sentences, temp = [], \"\"\n",
    "\n",
    "    for _, i  in enumerate(segments):\n",
    "        # with period\n",
    "        if not len(temp) and \".\" in i['text']:\n",
    "            \n",
    "            sentences.append({\n",
    "                'text': i['text'].strip(),\n",
    "                'start': round(i['start'], 3),\n",
    "                'end': round(i['end'], 3)\n",
    "            }) \n",
    "            \n",
    "            continue\n",
    "        # first condition fails\n",
    "        elif not len(temp):\n",
    "\n",
    "            temp, start = i['text'], round(i['start'], 3)\n",
    "\n",
    "            continue\n",
    "\n",
    "        temp += i['text']\n",
    "\n",
    "        if \".\" in i['text']:\n",
    "\n",
    "            sentences.append({\n",
    "                'text': temp.strip(),\n",
    "                'start': start,\n",
    "                'end': round(i['end'], 3)\n",
    "            })\n",
    "            temp = \"\"\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Good evening and welcome to Tucker Carlson tonight.',\n",
       "  'start': 0.0,\n",
       "  'end': 3.56},\n",
       " {'text': \"If there's one thing your average liberal understands perfectly well, it's that there's safety in numbers.\",\n",
       "  'start': 3.56,\n",
       "  'end': 10.08},\n",
       " {'text': \"Don't go out alone.\", 'start': 10.08, 'end': 11.48},\n",
       " {'text': 'Bring 80 million people with you.', 'start': 11.48, 'end': 13.28},\n",
       " {'text': \"It's safer that way.\", 'start': 13.28, 'end': 15.2},\n",
       " {'text': 'There is a reason, a fundamental reason, that Democrats are natural joiners and organizers and petition signers and that their highest virtue is conformity.',\n",
       "  'start': 15.2,\n",
       "  'end': 25.86},\n",
       " {'text': \"They know that as long as they're all wearing the same uniform, they'll probably be okay.\",\n",
       "  'start': 25.86,\n",
       "  'end': 30.74},\n",
       " {'text': \"This is why you'll see one person in Brookline or Bethesda raise a Ukrainian flag in the yard and the very next day, everybody on the street will have one too.\",\n",
       "  'start': 30.74,\n",
       "  'end': 40.58},\n",
       " {'text': \"Suddenly it's an entire neighborhood of foreign policy experts all specializing in Eastern European border disputes.\",\n",
       "  'start': 40.58,\n",
       "  'end': 47.3},\n",
       " {'text': \"It's amazing.\", 'start': 47.3, 'end': 48.72}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = get_sentences(data['de_fnc.mp4'])\n",
    "\n",
    "sentences[ : 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentence.json\", 'w') as f:\n",
    "    json.dump(sentences, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q yake  # yet another\n",
    "\n",
    "%pip install -q rake-nltk  # rapid automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "senten = \"This is why you'll see one person in Brookline or Bethesda raise a Ukrainian flag in the yard and the very next day, everybody on the street will have one too.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/karanjot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/karanjot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/karanjot/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['see one person',\n",
       " 'ukrainian flag',\n",
       " 'next day',\n",
       " 'bethesda raise',\n",
       " 'one',\n",
       " 'yard',\n",
       " 'street',\n",
       " 'everybody',\n",
       " 'brookline']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "\n",
    "rake = Rake()\n",
    "\n",
    "rake.extract_keywords_from_text(senten)\n",
    "words = rake.get_ranked_phrases()\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brookline',\n",
       " 'Bethesda',\n",
       " 'Ukrainian',\n",
       " 'day',\n",
       " 'person',\n",
       " 'raise',\n",
       " 'flag',\n",
       " 'yard',\n",
       " 'street']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "\n",
    "extractor = yake.KeywordExtractor()\n",
    "words = extractor.extract_keywords(senten)\n",
    "\n",
    "words = [i[0] for i in words if i[1] > 0.05]\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brookline', 'Bethesda']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "tagged_senten = pos_tag(senten.split())\n",
    "proper_nouns = [word for word, pos in tagged_senten if pos == 'NNP']\n",
    "\n",
    "proper_nouns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimum duration chunks with meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(sentence):\n",
    "    '''\n",
    "    fetch keywords and nouns\n",
    "    '''\n",
    "    extractor = yake.KeywordExtractor()\n",
    "    words = extractor.extract_keywords(sentence)\n",
    "\n",
    "    words = [i[0] for i in words if i[1] > 0.05 and len(i[0].split()) == 1]\n",
    "\n",
    "    tagged_senten = pos_tag(sentence.split())\n",
    "    \n",
    "    proper_nouns = [word for word, pos in tagged_senten if pos == 'NNP']\n",
    "\n",
    "    return [words, proper_nouns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(sentences, dur=5.0):\n",
    "  i, chunks = 0, []\n",
    "\n",
    "  while i < len(sentences):\n",
    "    # check duration\n",
    "    phrase, start, end = sentences[i]['text'], sentences[i]['start'], sentences[i]['end']\n",
    "\n",
    "    while end - start < dur:\n",
    "      i += 1\n",
    "      try:\n",
    "        phrase, end = phrase + ' ' + sentences[i]['text'], sentences[i]['end']\n",
    "      except IndexError: break\n",
    "\n",
    "    words, nouns = get_meta(phrase.strip())\n",
    "      \n",
    "    chunks.append({\n",
    "        'text': phrase.strip(),\n",
    "        'start': round(start, 3),\n",
    "        'end': round(end, 3),\n",
    "        'words': words,\n",
    "        'nouns': nouns\n",
    "    })\n",
    "    i += 1\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {'data': []}\n",
    "\n",
    "for i in [5.0, 10.0, 15.0, 20.0]:\n",
    "    \n",
    "    chunks['data'].append({\n",
    "        'duration': i,\n",
    "        'chunks': make_chunks(data['de_fnc.mp4'], dur=i)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Suddenly it's an entire neighborhood of foreign policy experts all specializing in Eastern  European border disputes.\",\n",
       " 'start': 40.58,\n",
       " 'end': 47.3,\n",
       " 'words': ['Eastern',\n",
       "  'European',\n",
       "  'Suddenly',\n",
       "  'disputes',\n",
       "  'entire',\n",
       "  'neighborhood',\n",
       "  'foreign',\n",
       "  'policy'],\n",
       " 'nouns': ['Eastern', 'European']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks['data'][0]['chunks'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunk.json\", 'w') as f:\n",
    "    json.dump(chunks, f, indent=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main: attain variations among transcript - phrase, sentence and chunk(duration-based), capture meta data- keywords and proper nouns(will act as targets for stance detection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
